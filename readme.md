#soundTextureGen
音频：
有两个方式。
1，预处理或者离线的，在播放BGM前，事先计算好BGM的整个频谱数据，有两个方法，在加载音频时计算，这个是耗时的，大概要10秒，在runtime由cpu根据time从频谱数据中取数据，通过uniform传入fsh。另一种是按shadertoy的方式，把数据离线处理成texture，可以用python写。格式上，由于fsh中只取了4个Freq，因而纹理的一个pixel就可以表示一个时间点的我们需要的那部分频谱。
更新：虽然shadertoy上音频是从纹理中采样的，但实际上它的这个纹理是逐帧用glBufferData更新的，只包含某时间点的频谱，而不是整个歌曲的。这和用uniform传没有区别。

一个1024的texture最大可以包含多长的数据？281分钟的4个采样点的频谱数据。1024*1024=1048576 = 60*60*281

1，用python计算某个时间点的音乐的频谱。
2，写入无压缩的PNG文件。

注意，runtime的音频播放和texture进度的同步。

2，实时的，直接从即将送入播放的数据中获取PCM来FFT出频谱来，这样可以不考虑时间上的同步等问题；但实际上很难做到，因为声音数据在实际播放时并不是经过CPU的，而是由CPU控制类似DMA的东西来播放的，而且可能牵涉到多线程的问题，所以可行性低。